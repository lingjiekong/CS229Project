{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load training result get from preprocess.py module\n",
    "\n",
    "import json\n",
    "with open('ad2Adv.json', 'r') as fp:\n",
    "    ad2AdvStr = json.load(fp)\n",
    "\n",
    "with open('ad2Cam.json', 'r') as fp:\n",
    "    ad2CamStr = json.load(fp)\n",
    "\n",
    "with open('finalDis2Doc.json', 'r') as fp:\n",
    "    finalDis2DocStr = json.load(fp)\n",
    "    \n",
    "with open('finalDoc2Cate.json', 'r') as fp:\n",
    "    finalDoc2CateStr = json.load(fp) \n",
    "\n",
    "with open('finalDoc2Topic.json', 'r') as fp:\n",
    "    finalDoc2TopicStr = json.load(fp)  \n",
    "    \n",
    "with open('finalDoc2Meta.json', 'r') as fp:\n",
    "    finalDoc2MetaStr = json.load(fp)  \n",
    "    \n",
    "with open('finalData.json', 'r') as fp:\n",
    "    finalDataStr = json.load(fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert str dict to value dict\n",
    "ad2Adv = dict()\n",
    "for each in ad2AdvStr:\n",
    "    ad2Adv[float(each)] = float(ad2AdvStr[each])\n",
    "\n",
    "ad2Cam = dict()\n",
    "for each in ad2CamStr:\n",
    "    ad2Cam[float(each)] = float(ad2CamStr[each])\n",
    "    \n",
    "finalDis2Doc = dict()\n",
    "for each in finalDis2DocStr:\n",
    "    finalDis2Doc[float(each)] = float(finalDis2DocStr[each])\n",
    "    \n",
    "finalDoc2Cate = dict()\n",
    "for each in finalDoc2CateStr:\n",
    "    finalDoc2Cate[float(each)] = float(finalDoc2CateStr[each])\n",
    "    \n",
    "finalDoc2Topic = dict()\n",
    "for each in finalDoc2TopicStr:\n",
    "    finalDoc2Topic[float(each)] = float(finalDoc2TopicStr[each])\n",
    "    \n",
    "finalDoc2Meta = dict()\n",
    "for each in finalDoc2MetaStr:\n",
    "    finalDoc2Meta[float(each)] = float(finalDoc2MetaStr[each])\n",
    "    \n",
    "finalData = dict()\n",
    "for each in finalDataStr:\n",
    "    finalData[float(each)] = eval(finalDataStr[each])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare set x and y\n",
    "import numpy as np\n",
    "x = np.array\n",
    "xTrainSet = []\n",
    "yTrainSet = []\n",
    "xTestSet = []\n",
    "yTestSet = []\n",
    "# this is only use for test\n",
    "xTrainClickSet = []\n",
    "yTrainClickSet = []\n",
    "xTestClickSet = []\n",
    "yTestClickSet = []\n",
    "xTestNotClickSet = []\n",
    "yTestNotClickSet = []\n",
    "\n",
    "# 1 click and 1 not click count\n",
    "clickCount = 0\n",
    "nonClickCount = 0\n",
    "\n",
    "countSplit = 0\n",
    "splitThreshold = 2702\n",
    "for disID in finalData:\n",
    "    xSubSet = []\n",
    "    ySubSet = []\n",
    "    adClickIDTupleSet = finalData[disID]\n",
    "    for adClickIDTuple in adClickIDTupleSet:\n",
    "        adID = adClickIDTuple[0]\n",
    "        clickID = adClickIDTuple[1]\n",
    "        # from ad_id\n",
    "        advID = ad2Adv[adID]\n",
    "        camID = ad2Cam[adID]\n",
    "        # docID key\n",
    "        docID = finalDis2Doc[disID]\n",
    "        # from display_id\n",
    "        cateID = finalDoc2Cate[docID]\n",
    "        topicID = finalDoc2Topic[docID]\n",
    "        metaID = finalDoc2Meta[docID]\n",
    "        # make subset for x\n",
    "        xSubSet = [advID, camID, cateID, topicID, metaID]\n",
    "        # make subset for y\n",
    "        if (clickID == 0): # not click\n",
    "            ySubSet = [1, 0]\n",
    "        else:\n",
    "            ySubSet = [0, 1] # click\n",
    "        # train and test dict\n",
    "        if (countSplit < splitThreshold):\n",
    "            xTrainSet.append(xSubSet)\n",
    "            yTrainSet.append(ySubSet) \n",
    "            if (ySubSet == [0, 1]) and clickCount == 0: # click\n",
    "                xTrainClickSet.append(xSubSet)\n",
    "                yTrainClickSet.append(ySubSet)\n",
    "                clickCount += 1\n",
    "            if (ySubSet == [1, 0]) and nonClickCount == 0: # not click\n",
    "                xTrainClickSet.append(xSubSet)\n",
    "                yTrainClickSet.append(ySubSet)\n",
    "                nonClickCount += 1\n",
    "        else:\n",
    "            xTestSet.append(xSubSet)\n",
    "            yTestSet.append(ySubSet) \n",
    "            if (ySubSet == [0, 1]): # click\n",
    "                xTestClickSet.append(xSubSet)\n",
    "                yTestClickSet.append(ySubSet)\n",
    "            else:\n",
    "                xTestNotClickSet.append(xSubSet)\n",
    "                yTestNotClickSet.append(ySubSet)\n",
    "    countSplit += 1\n",
    "    clickCount = 0\n",
    "    nonClickCount = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13676\n",
      "5404\n",
      "2911\n",
      "2911\n"
     ]
    }
   ],
   "source": [
    "print (len(xTrainSet))\n",
    "print (len(xTrainClickSet))\n",
    "print (len(xTestSet))\n",
    "print (len(xTestClickSet)+len(xTestNotClickSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train\n",
    "# convert set x and y to matrix x and y\n",
    "xTrain = np.array(xTrainSet, np.int32)\n",
    "yTrain = np.array(yTrainSet, np.int32)\n",
    "# convert set x and y to matrix x and y\n",
    "xTrainClick = np.array(xTrainClickSet, np.int32)\n",
    "yTrainClick = np.array(yTrainClickSet, np.int32)\n",
    "\n",
    "# test\n",
    "# convert set x and y to matrix x and y\n",
    "xTest = np.array(xTestSet, np.int32)\n",
    "yTest = np.array(yTestSet, np.int32)\n",
    "# convert set x and y to matrix x and y\n",
    "xTestClick = np.array(xTestClickSet, np.int32)\n",
    "yTestClick = np.array(yTestClickSet, np.int32)\n",
    "# convert set x and y to matrix x and y\n",
    "xTestNotClick = np.array(xTestNotClickSet, np.int32)\n",
    "yTestNotClick = np.array(yTestNotClickSet, np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make dict for each display_id to matrix for passing to tensorflow for training\n",
    "xTestDict = dict()\n",
    "yTestDict = dict()\n",
    "start = 0\n",
    "count = 0\n",
    "for i in range(len(xTest)-1):\n",
    "    if xTest[i,2] != xTest[i+1,2]:\n",
    "        xSubMatrix = xTest[start:i+1]\n",
    "        ySubMatrix = yTest[start:i+1]\n",
    "        xTestDict[count] = xSubMatrix\n",
    "        yTestDict[count] = ySubMatrix\n",
    "        count += 1\n",
    "        start = i+1\n",
    "        \n",
    "xTrainDict = dict()\n",
    "yTrainDict = dict()\n",
    "start = 0\n",
    "count = 0\n",
    "for i in range(len(xTrain)-1):\n",
    "    if xTrain[i,2] != xTrain[i+1,2]:\n",
    "        xSubMatrix = xTrain[start:i+1]\n",
    "        ySubMatrix = yTrain[start:i+1]\n",
    "        xTrainDict[count] = xSubMatrix\n",
    "        yTrainDict[count] = ySubMatrix\n",
    "        count += 1\n",
    "        start = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560\n",
      "2560\n",
      "534\n",
      "534\n"
     ]
    }
   ],
   "source": [
    "print (len(xTrainDict))\n",
    "print (len(yTrainDict))\n",
    "print (len(xTestDict))\n",
    "print (len(yTestDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-65-2a79d32ed1e9>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-65-2a79d32ed1e9>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    from sklearn.neural_network\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.539993192\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feed_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-5a2b4b3db7bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mxTestDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feed_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# start Multilayer Perceptron NN \n",
    "# use tensorflow NN with Multilayer Perceptron to train\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "# Import preprocessed data\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 1\n",
    "total_batch = len(xTrainDict) # m: sample\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 300 # 1st layer number of features\n",
    "n_hidden_2 = 300 # 2nd layer number of features\n",
    "n_input = 5 # (advID, camID, cateID, topicID, metaID) # n: x (# of feature) \n",
    "n_classes = 2 # click or not click (0, 1) # y\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.tanh(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.tanh(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "# init = tf.initialize_all_variables()\n",
    "# this is a new method of initialization\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x = xTrainDict[i]\n",
    "            batch_y = yTrainDict[i]           \n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    # check accuracy\n",
    "    totalCount = 0\n",
    "    correctCount = 0\n",
    "    for i in range(len(xTestDict)):\n",
    "        totalCount += 1\n",
    "        \n",
    "        \n",
    "        _, loss_value = sess.run([xTestDict[0], cost], feed_dict=feed_dict)\n",
    "        print (loss_value)\n",
    "        \n",
    "        \n",
    "        if accuracy.eval({x: xTestDict[i], y: yTestDict[i]}) == 1:\n",
    "            correctCount += 1\n",
    "    print(\"Accuracy:\", float(correctCount)/float(totalCount))\n",
    "#     print(\"Accuracy:\", accuracy.eval({x: xTest, y: yTest}))\n",
    "#     print(\"Accuracy Click:\", accuracy.eval({x: xTestClick, y: yTestClick}))\n",
    "#     print(\"Accuracy Not Click:\", accuracy.eval({x: xTestNotClick, y: yTestNotClick}))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        # Loop over all batches\n",
    "        for i in range(len(xTrainDict)):\n",
    "            batch_x = xTrainDict[i]\n",
    "            batch_y = yTrainDict[i]   \n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    # check accuracy \n",
    "    totalCount = 0\n",
    "    correctCount = 0\n",
    "    for i in range(len(xTestDict)):\n",
    "        totalCount += 1\n",
    "        if accuracy.eval({x: xTestDict[i], y: yTestDict[i]}) == 1:\n",
    "            correctCount += 1\n",
    "    print(\"Accuracy:\", float(correctCount)/float(totalCount))\n",
    "#     print(\"Accuracy:\", accuracy.eval({x: xTest, y: yTest}))\n",
    "#     print(\"Accuracy Click:\", accuracy.eval({x: xTestClick, y: yTestClick}))\n",
    "#     print(\"Accuracy Not Click:\", accuracy.eval({x: xTestNotClick, y: yTestNotClick}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Session' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-1e8e4f9208c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Session' object has no attribute 'eval'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# A Multilayer Perceptron implementation example using TensorFlow library.\n",
    "# This example is using the MNIST database of handwritten digits\n",
    "# (http://yann.lecun.com/exdb/mnist/)\n",
    "# Author: Aymeric Damien\n",
    "# Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "# '''\n",
    "\n",
    "# from __future__ import print_function\n",
    "\n",
    "# # Import MNIST data\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # Parameters\n",
    "# learning_rate = 0.001\n",
    "# training_epochs = 15\n",
    "# batch_size = 100 # m: sample\n",
    "# display_step = 1\n",
    "\n",
    "# # Network Parameters\n",
    "# n_hidden_1 = 256 # 1st layer number of features\n",
    "# n_hidden_2 = 256 # 2nd layer number of features\n",
    "# n_input = 784 # MNIST data input (img shape: 28*28) # n: x (# of feature) \n",
    "# n_classes = 10 # MNIST total classes (0-9 digits) # y\n",
    "\n",
    "# # tf Graph input\n",
    "# x = tf.placeholder(\"float\", [None, n_input])\n",
    "# y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "\n",
    "# # Create model\n",
    "# def multilayer_perceptron(x, weights, biases):\n",
    "#     # Hidden layer with RELU activation\n",
    "#     layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "#     layer_1 = tf.nn.relu(layer_1)\n",
    "#     # Hidden layer with RELU activation\n",
    "#     layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "#     layer_2 = tf.nn.relu(layer_2)\n",
    "#     # Output layer with linear activation\n",
    "#     out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "#     return out_layer\n",
    "\n",
    "# # Store layers weight & bias\n",
    "# weights = {\n",
    "#     'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "#     'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "#     'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "# }\n",
    "# biases = {\n",
    "#     'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "#     'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "#     'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "# }\n",
    "\n",
    "# # Construct model\n",
    "# pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# # Define loss and optimizer\n",
    "# cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "# # cost = tf.reduce_mean(tf.nn.sigmoid(pred, y))\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# # Initializing the variables\n",
    "# # init = tf.initialize_all_variables()\n",
    "# init = tf.global_variables_initializer()\n",
    "\n",
    "# # Launch the graph\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "\n",
    "#     # Training cycle\n",
    "#     for epoch in range(training_epochs):\n",
    "#         avg_cost = 0.\n",
    "#         total_batch = int(mnist.train.num_examples/batch_size)\n",
    "#         print (float(mnist.train.num_examples))\n",
    "#         # Loop over all batches\n",
    "#         for i in range(total_batch):\n",
    "#             batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "#             # Run optimization op (backprop) and cost op (to get loss value)\n",
    "#             _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "#                                                           y: batch_y})\n",
    "#             # Compute average loss\n",
    "#             avg_cost += c / total_batch\n",
    "#         # Display logs per epoch step\n",
    "#         if epoch % display_step == 0:\n",
    "#             print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "#                 \"{:.9f}\".format(avg_cost))\n",
    "#     print(\"Optimization Finished!\")\n",
    "\n",
    "#     # Test model\n",
    "#     correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "#     # Calculate accuracy\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "#     print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
